import portable.option.option
import experiments.rainbow_experiment

RainbowExperiment.experiment_name = "rainbow_monte"
RainbowExperiment.initiation_vote_function = "weighted_vote_low"
RainbowExperiment.termination_vote_function = "weighted_vote_low"
RainbowExperiment.device_type = "cuda"
RainbowExperiment.train_initiation = False
RainbowExperiment.train_termination = False
RainbowExperiment.train_policy = False
RainbowExperiment.train_initiation_classifier_epochs = 150
RainbowExperiment.train_initiation_embedding_epochs = 60
RainbowExperiment.train_termination_classifier_epochs = 150
RainbowExperiment.train_termination_embedding_epochs = 60
RainbowExperiment.train_policy_max_steps = 2000000
RainbowExperiment.train_policy_success_rate = 0.9
RainbowExperiment.primitive_action_num = 18
RainbowExperiment.action_num = 40

Option.prioritized_replay_anneal_steps = 50000
Option.policy_warmup_steps = 1024
Option.q_target_update_interval = 10
Option.policy_buffer_length = 100000
Option.timeout = 500
Option.min_interactions = 100
Option.initiation_embedding_learning_rate = 1e-4
Option.termination_embedding_learning_rate = 1e-4
Option.initiation_beta_distribution_alpha = 100
Option.initiation_beta_distribution_beta = 10
Option.termination_beta_distribution_alpha = 100
Option.termination_beta_distribution_beta = 10