import experiments.minigrid.advanced_doorkey.core.advanced_minigrid_factored_experiment
import portable.option.portable_option
import portable.option.markov.nn_markov_option
import portable.option.ensemble.custom_attention

# FLAGS 
AdvancedMinigridFactoredExperiment.make_videos                      = True
AttentionOption.update_options_from_success                         = False
AttentionOption.create_instances                                    = False
AttentionOption.assume_initiation_start                             = True

AdvancedMinigridFactoredExperiment.experiment_name                  = "perfect-mask-classifier"
AdvancedMinigridFactoredExperiment.policy_lr                        = 1e-4
AdvancedMinigridFactoredExperiment.policy_max_steps                 = 2e6
AdvancedMinigridFactoredExperiment.policy_success_threshold         = 0.9
AdvancedMinigridFactoredExperiment.use_gpu                          = True
AdvancedMinigridFactoredExperiment.num_instances_per_option         = 20

AttentionOption.initiation_vote_threshold                           = 0.4
AttentionOption.termination_vote_threshold                          = 0.4
AttentionOption.prioritized_replay_anneal_steps                     = 50000
AttentionOption.policy_warmup_steps                                 = 1024
AttentionOption.policy_batchsize                                    = 32
AttentionOption.policy_buffer_length                                = 50000
AttentionOption.policy_update_interval                              = 4
AttentionOption.q_target_update_interval                            = 10
AttentionOption.policy_learning_rate                                = 2.5e-4
AttentionOption.final_epsilon                                       = 0.01
AttentionOption.final_exploration_frames                            = 1e6
AttentionOption.discount_rate                                       = 0.9
AttentionOption.policy_attention_module_num                         = 1
AttentionOption.num_actions                                         = 7
AttentionOption.policy_c                                            = 100
AttentionOption.initiation_beta_distribution_alpha                  = 100
AttentionOption.initiation_beta_distribution_beta                   = 10
AttentionOption.initiation_attention_module_num                     = 1
AttentionOption.initiation_lr                                       = 1e-4
AttentionOption.initiation_dataset_maxsize                          = 100000
AttentionOption.termination_beta_distribution_alpha                 = 100
AttentionOption.termination_beta_distribution_beta                  = 10
AttentionOption.termination_attention_module_num                    = 1
AttentionOption.termination_lr                                      = 1e-4
AttentionOption.termination_dataset_maxsize                         = 100000
AttentionOption.min_interactions                                    = 100
AttentionOption.min_success_rate                                    = 0.7
AttentionOption.timeout                                             = 50
AttentionOption.min_option_length                                   = 1
AttentionOption.max_instantiations                                  = 20
AttentionOption.option_handling_method                              = "continue-with-multi-instantiations"
AttentionOption.policy_divergence_loss_scale                        = 0

FactoredAttentionLayer.num_features                                 = 10
FactoredAttentionLayer.expansion_amount                             = [4,2,2,2,2,2,2,3,2,1]
FactoredAttentionLayer.mask_parameters                              = [1,0,1,0,0,0,0,0,0,0]

MockAutoEncoder.feature_size                                        = 22