import portable.agent.model.linear_q
import experiments.divdis_minigrid.core.advanced_minigrid_factored_divdis_meta_experiment
import portable.agent.option_agent
import portable.option.divdis.divdis_mock_option
import portable.option.divdis.policy.policy_and_initiation

LargeLinearQFunction.in_features                                    = 22
LargeLinearQFunction.n_actions                                      = 7

OptionLinearQFunction.in_features                                   = 22
OptionLinearQFunction.action_vector_size                            = 10
OptionLinearQFunction.num_options                                   = 2

FactoredAdvancedMinigridDivDisMetaExperiment.experiment_name        = "meta_policy_trained_options"
FactoredAdvancedMinigridDivDisMetaExperiment.num_options            = 3
FactoredAdvancedMinigridDivDisMetaExperiment.num_primitive_actions  = 7
FactoredAdvancedMinigridDivDisMetaExperiment.use_gpu                = True

OptionAgent.num_actions                                             = 13
OptionAgent.batch_size                                              = 32
OptionAgent.warmup_steps                                            = 1024
OptionAgent.buffer_length                                           = 50000
OptionAgent.update_interval                                         = 4
OptionAgent.q_target_update_interval                                = 10
OptionAgent.learning_rate                                           = 2.5e-4
OptionAgent.final_epsilon                                           = 0.05
OptionAgent.final_exploration_frames                                = 1e6
OptionAgent.discount_rate                                           = 0.9
OptionAgent.prioritized_replay_anneal_steps                         = 500000

PolicyWithInitiation.warmup_steps                                   = 1024
PolicyWithInitiation.prioritized_replay_anneal_steps                = 50000
PolicyWithInitiation.buffer_length                                  = 50000
PolicyWithInitiation.update_interval                                = 4
PolicyWithInitiation.q_target_update_interval                       = 10
PolicyWithInitiation.learning_rate                                  = 2.5e-4
PolicyWithInitiation.final_epsilon                                  = 0.01
PolicyWithInitiation.final_exploration_frames                       = 5e4
PolicyWithInitiation.batch_size                                     = 32
PolicyWithInitiation.num_actions                                    = 7
PolicyWithInitiation.policy_infeature_size                          = 26
PolicyWithInitiation.q_hidden_size                                  = 64
PolicyWithInitiation.gru_hidden_size                                = 128
