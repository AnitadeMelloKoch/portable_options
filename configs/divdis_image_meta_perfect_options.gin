import portable.agent.model.linear_q
import experiments.core.divdis_meta_masked_ppo_experiment
import portable.option.divdis.divdis_mock_option
import portable.option.divdis.policy.policy_and_initiation
import portable.option.divdis.policy.skill_ppo
import portable.agent.model.maskable_ppo 

DivDisMetaMaskedPPOExperiment.experiment_name                       = "meta_image_policy_perfect_options"
DivDisMetaMaskedPPOExperiment.num_options                           = 7
DivDisMetaMaskedPPOExperiment.num_primitive_actions                 = 0
DivDisMetaMaskedPPOExperiment.use_gpu                               = True
DivDisMetaMaskedPPOExperiment.make_videos                           = True
DivDisMetaMaskedPPOExperiment.option_type                           = "mock"
DivDisMetaMaskedPPOExperiment.option_timeout                        = 150

MaskablePPOAgent.learning_rate                                      = 2.5e-4
MaskablePPOAgent.state_shape                                        = (3, 84, 84)
MaskablePPOAgent.num_actions                                        = 7

PolicyWithInitiation.warmup_steps                                   = 1024
PolicyWithInitiation.prioritized_replay_anneal_steps                = 500000
PolicyWithInitiation.buffer_length                                  = 200000
PolicyWithInitiation.update_interval                                = 4
PolicyWithInitiation.q_target_update_interval                       = 10
PolicyWithInitiation.learning_rate                                  = 2.5e-4
PolicyWithInitiation.final_epsilon                                  = 0.01
PolicyWithInitiation.final_exploration_frames                       = 8e5
PolicyWithInitiation.batch_size                                     = 32
PolicyWithInitiation.num_actions                                    = 7
PolicyWithInitiation.policy_infeature_size                          = 11552
PolicyWithInitiation.q_hidden_size                                  = 128
PolicyWithInitiation.gru_hidden_size                                = 512
PolicyWithInitiation.image_input                                    = True

SkillPPO.learning_rate                                              = 2.5e-4
SkillPPO.state_shape                                                = (3, 84, 84)
SkillPPO.num_actions                                                = 7 
SkillPPO.entropy_coef                                               = 0.01
SkillPPO.minibatch_size                                             = 256

DivDisMockOption.exp_type                                           = "minigrid"
DivDisMockOption.tabular_beta                                       = 0